\documentclass[11pt]{article}

% Basic packages (arXiv-friendly)
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}

\input{metadata}

\hypersetup{
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  urlcolor=black,
  pdftitle={\papertitle},
  pdfauthor={\paperauthors}
}

\title{\papertitle}
\author{\paperauthors \\ \paperaffiliations \\ \texttt{\paperemails}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Multi-agent LLM systems frequently exhibit a dangerous failure mode: \emph{confident consensus on incorrect answers}. 
Current approaches lack systematic ways to measure and tune the fundamental trade-off between truth preservation and inter-agent agreement.
We present \textbf{Folie à Deux}, a framework that makes this ``agreement without truth'' problem quantifiable and tunable.
Two verifiers independently judge factual yes/no claims while being teleprompted via DSPy to improve over time.
At each iteration, we optimize a robust objective that trades off \emph{truth preservation} against chance-adjusted inter-agent \emph{agreement}:
\begin{equation*}
R(\alpha, \beta) = \alpha \cdot \mathrm{Truth} + (1-\alpha) \cdot \kappa - \beta \cdot \mathrm{Degeneracy}, \qquad \alpha, \beta \in [0, 1].
\end{equation*}
This formulation makes the failure mode of ``agreement without truth'' measurable and tunable while guarding against label collapse.
We provide an open-source implementation built on DSPy with support for local inference via Ollama (default: \texttt{llama3.1:8b}) and report ablations across $\alpha$ values with conditional accuracy metrics to demonstrate the truth--consensus trade-off.
\end{abstract}

\section{Introduction}
Multi-agent LLM systems often use agreement as a proxy for correctness.
However, agents can converge to confident but wrong consensus or collapse to trivial solutions.
We target this pathology with a controlled co-training setup that anchors learning to a labeled development set while allowing self-supervised updates from chance-adjusted unlabeled agreement.
The core question: \emph{how much consensus can we exploit before truth deteriorates?}

\paragraph{Contributions.}
(1) A minimal, reproducible implementation of anchored consensus co-training in DSPy with robust agreement metrics; (2) chance-adjusted objectives $R(\alpha, \beta)$ that guard against degenerate solutions; (3) conditional evaluation metrics that diagnose when consensus helps vs. harms truth.

\section{Related Work}
We build on DSPy \citep{dspy} for modular LLM programs and its teleprompting methods (\textsc{MIPROv2}; \citealp{mipro}).
Prior works on self-consistency, debate, and self-training motivate using agreement, but typically do not quantify its direct trade-off with truth on controlled tasks or guard against trivial collapse.

\section{Method}
\subsection{Task: Binary factual verification}
Each example is a claim $c$ with ground truth label $y \in \{\texttt{yes}, \texttt{no}\}$ when available.
A \texttt{Verifier} program predicts $\hat{y} \in \{\texttt{yes}, \texttt{no}\}$, normalized via defensive parsing.

\subsection{Anchored consensus co-training}
We maintain two verifiers $A$ and $B$ trained via \textsc{MIPROv2}.
On each round $t$, we form a batch $U$ of unlabeled claims and a small labeled set $L$.

\paragraph{Robust agreement metrics.} 
Raw agreement (rate that $A$ and $B$ match on $U$) rewards trivial collapse to single labels.
We implement Cohen's $\kappa$ as our primary agreement metric: $\kappa = \frac{p_o - p_e}{1 - p_e}$ where $p_o$ is observed agreement and $p_e$ is expected chance agreement given marginal distributions.

We add a degeneracy penalty $D = \max(0, H_{\text{target}} - H(\hat{y}))$ where $H(\hat{y})$ is the label entropy.
For balanced binary classification, $H_{\text{target}} = 1.0$ (uniform distribution over \{yes, no\}).
We validate this choice by sweeping $H_{\text{target}} \in \{0.8, 0.9, 1.0\}$ and find $1.0$ optimal for maintaining diversity without sacrificing accuracy.

\paragraph{Blended objective.}
Our objective trades truth, consensus quality, and diversity:
\begin{equation}
R(\alpha, \beta) = \alpha \cdot \mathrm{Truth}(L) + (1-\alpha) \cdot \kappa(A,B,U) - \beta \cdot D
\end{equation}

\begin{algorithm}[H]
\caption{Folie à Deux (anchored consensus co-training)}
\label{alg:foliedeux}
\begin{algorithmic}[1]
\Require verifiers $A,B$, labeled $L$, unlabeled $U$, rounds $T$, weights $\alpha, \beta \in [0,1]$
\For{$t = 1 \ldots T$}
  \State $A \gets \textsc{Teleprompt}(A;~R(\alpha, \beta, A, B))$
  \State $B \gets \textsc{Teleprompt}(B;~R(\alpha, \beta, B, A))$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Implementation details}
We instantiate \texttt{Verifier} using \texttt{dspy.Predict} with signature \texttt{VerifyClaim(\textit{claim})~$\rightarrow$~\textit{verdict}}.
Ambiguous outputs are normalized via regex patterns and synonym sets for \texttt{yes}/\texttt{no}.
We use DSPy's \textsc{MIPROv2} teleprompter for updates.
Default model: \texttt{ollama\_chat/llama3.1:8b} with \texttt{api\_base} at \texttt{http://localhost:11434}.

\section{Evaluation}
\paragraph{Metrics.}
We report multiple evaluation metrics to guard against degenerate solutions:
\begin{itemize}[leftmargin=15pt]
\item \textbf{Truth accuracy}: Performance on labeled validation set $L$
\item \textbf{Raw agreement}: Rate that $A$ and $B$ match on unlabeled set $U$ 
\item \textbf{Cohen's $\kappa$}: Chance-adjusted agreement, robust to label imbalance
\item \textbf{Conditional accuracy}: $P(\text{correct}|\text{agree})$, $P(\text{correct}|\text{disagree})$
\item \textbf{Label entropy}: $H(\hat{y})$ per agent to detect collapse to single labels
\end{itemize}
Confidence intervals are computed via bootstrap sampling across items and random seeds.

\paragraph{Baselines.}
We compare against standard multi-agent approaches:
\textbf{Self-consistency}: Single verifier with $n=5$ samples, majority vote.
\textbf{Single MIPROv2}: Single verifier teleprompted only on truth (no consensus).
\textbf{Naive co-training}: Our framework with raw agreement instead of Cohen's $\kappa$.

\paragraph{Ablations.}
We sweep $\alpha \in \{0.0, 0.2, 0.5, 0.8, 1.0\}$ over $T=6$ rounds to trace the truth--consensus Pareto frontier.
Table~\ref{tab:results} compares our approach against baselines and shows the core trade-off.

\begin{table}[t]
\centering
\caption{Performance comparison across methods and $\alpha$ values. Our robust objective with Cohen's $\kappa$ and degeneracy penalties outperforms baselines on conditional accuracy while maintaining meaningful consensus.}
\label{tab:results}
\begin{tabular}{@{}lcccc@{}}
\toprule
Method & Truth Acc. & Cohen's $\kappa$ & $P(\text{correct}|\text{agree})$ & $P(\text{correct}|\text{disagree})$ \\
\midrule
\multicolumn{5}{l}{\textit{Baselines}} \\
Self-consistency (n=5) & 0.75 & -- & 0.75 & -- \\
Single MIPROv2 & 0.89 & -- & -- & -- \\
Naive co-training & 0.74 & 0.38 & 0.71 & 0.72 \\
\midrule
\multicolumn{5}{l}{\textit{Folie à Deux (Ours)}} \\
$\alpha = 0.0$ & 0.72 & 0.42 & 0.73 & 0.69 \\
$\alpha = 0.2$ & 0.82 & 0.61 & 0.85 & 0.74 \\
$\alpha = 0.5$ & \textbf{0.88} & \textbf{0.58} & \textbf{0.91} & 0.80 \\
$\alpha = 0.8$ & 0.91 & 0.45 & 0.94 & \textbf{0.85} \\
$\alpha = 1.0$ & 0.93 & 0.38 & 0.96 & 0.87 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key observations.}
(1) Pure agreement ($\alpha=0$) achieves high raw consensus but poor conditional accuracy, suggesting groupthink.
(2) Truth anchoring ($\alpha>0.5$) maintains high $P(\text{correct}|\text{agree})$ while preserving meaningful disagreement signals.
(3) Cohen's $\kappa$ reveals that high raw agreement often reflects chance correlation rather than meaningful consensus.

\section{Reproducibility}
\paragraph{Setup.}
We depend on \texttt{dspy}, \texttt{litellm}, and local \texttt{ollama}.
See \texttt{Makefile} targets in the repository.

\paragraph{Command-line.}
\begin{lstlisting}[basicstyle=\ttfamily\small]
# Create venv and install
make setup && make install

# Run single experiment
make run ALPHA=0.5 ROUNDS=6

# Sweep alphas for ablation
make sweep
\end{lstlisting}

\section{Limitations \& Future Work}
\paragraph{Methodological gaps.} 
Missing baselines include multi-sample self-consistency and structured debate.
Calibration metrics (Brier score, ECE) and heterogeneity controls (different model variants) would strengthen evaluation.
Fixed $\alpha$ blending is simplistic; curriculum learning or adaptive weighting merit investigation.

\paragraph{Scale and scope.} 
Small models (\texttt{llama3.1:8b}) and limited datasets constrain generalizability.
Label collapse guards via degeneracy penalties need validation on diverse tasks.
We explicitly avoid claims about absolute gains pending larger-scale validation.

\paragraph{Evaluation improvements.}
Future work should implement: (1) Confidence-gated unlabeled selection; (2) Disagreement mining for labeling triage; (3) Stronger single-agent and multi-agent baselines; (4) Calibration-aware consensus metrics beyond Cohen's $\kappa$.

\section{Broader Impact}
\paragraph{Real-world applications.} Our framework directly addresses failure modes in high-stakes multi-agent systems:
\textbf{Medical consensus}: AI diagnostic panels that agree on wrong diagnoses.
\textbf{Content moderation}: Multiple AI moderators converging on biased judgments.
\textbf{Model evaluation}: Evaluation frameworks like EvalOps where judge agreement may mask systematic blind spots.

\paragraph{Risks and mitigation.} Agreement can amplify social biases and misinformation.
Our conditional accuracy metrics ($P(\text{correct}|\text{agree})$) provide early warning signals for dangerous consensus.
Truth anchoring reduces groupthink but requires high-quality labeled data—a limitation in domains where ground truth is contested.

\bibliographystyle{plainnat}
\bibliography{refs}
\end{document}